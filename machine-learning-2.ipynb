{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44b15383-63ea-4a78-8bdc-8c3507a0d256",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2037273622.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    Overfitting occurs when a model is too complex and learns the noise in the training data, resulting in poor performance on new, unseen data. Consequences of overfitting include:\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\n",
    "\n",
    "Overfitting occurs when a model is too complex and learns the noise in the training data, resulting in poor performance on new, unseen data. Consequences of overfitting include:\n",
    "\n",
    "Poor generalization to new data\n",
    "High variance in model performance\n",
    "Mitigation strategies for overfitting include:\n",
    "\n",
    "Regularization techniques (e.g., L1, L2)\n",
    "Early stopping\n",
    "Data augmentation\n",
    "Model simplification\n",
    "Underfitting occurs when a model is too simple and fails to capture the underlying patterns in the training data, resulting in poor performance on both training and testing data. Consequences of underfitting include:\n",
    "\n",
    "Poor performance on both training and testing data\n",
    "High bias in model performance\n",
    "Mitigation strategies for underfitting include:\n",
    "\n",
    "Increasing model complexity\n",
    "Collecting more training data\n",
    "Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "584253df-3f10-4c74-96d7-68e889fe73f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (744491841.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Q2: How can we reduce overfitting? Explain in brief.\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief.\n",
    "\n",
    "To reduce overfitting, we can use regularization techniques such as L1 (Lasso) or L2 (Ridge) regularization, which add a penalty term to the loss function to discourage large model coefficients. We can also use early stopping, data augmentation, and model simplification to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05a8aa6a-7820-4811-8b85-9319b61b9dcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4115749047.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "\n",
    "Underfitting occurs when a model is too simple and fails to capture the underlying patterns in the training data. Scenarios where underfitting can occur in ML include:\n",
    "\n",
    "Using a linear model for a non-linear problem\n",
    "Having too few features or inputs\n",
    "Having too little training data\n",
    "Using a model with too few parameters or complexity\n",
    "Not using feature engineering or selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a6afdb4-d0bb-4084-8a0b-b671a2ae0272",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2595580792.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    The bias-variance tradeoff refers to the tradeoff between the error introduced by simplifying a model (bias) and the error introduced by fitting the noise in the data (variance). A model with high bias pays little attention to the training data and oversimplifies, resulting in poor performance. A model with high variance pays too much attention to the training data and overfits, resulting in poor performance. The goal is to find a balance between bias and variance to achieve good model performance.\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?\n",
    "\n",
    "The bias-variance tradeoff refers to the tradeoff between the error introduced by simplifying a model (bias) and the error introduced by fitting the noise in the data (variance). A model with high bias pays little attention to the training data and oversimplifies, resulting in poor performance. A model with high variance pays too much attention to the training data and overfits, resulting in poor performance. The goal is to find a balance between bias and variance to achieve good model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f68fc378-e6dd-41bd-adcf-7d5cda669db7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 7) (3219978532.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    Analyzing the model's complexity: if the model has too many parameters or is too complex, it may indicate overfitting. If the model is too simple, it may indicate underfitting.\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 7)\n"
     ]
    }
   ],
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?\n",
    "\n",
    "Common methods for detecting overfitting and underfitting include:\n",
    "\n",
    "Plotting the training and testing error curves: if the training error decreases while the testing error increases, it may indicate overfitting. If the training error is high and the testing error is also high, it may indicate underfitting.\n",
    "Using cross-validation: if the model performs well on the training data but poorly on the validation data, it may indicate overfitting.\n",
    "Analyzing the model's complexity: if the model has too many parameters or is too complex, it may indicate overfitting. If the model is too simple, it may indicate underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb134525-1ff6-4626-9761-92cd83426a27",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2930266282.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    Bias refers to the error introduced by simplifying a model, while variance refers to the error introduced by fitting the noise in the data. High bias models oversimplify and have poor performance, while high variance models overfit and have poor performance.\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?\n",
    "\n",
    "Bias refers to the error introduced by simplifying a model, while variance refers to the error introduced by fitting the noise in the data. High bias models oversimplify and have poor performance, while high variance models overfit and have poor performance.\n",
    "\n",
    "Examples of high bias models include:\n",
    "\n",
    "Linear models for non-linear problems\n",
    "Decision trees with limited depth\n",
    "Examples of high variance models include:\n",
    "\n",
    "Decision trees with high depth\n",
    "Neural networks with many layers and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81095217-a59a-4d21-be27-b488423d90de",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 8) (1811620593.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    Early stopping: stops training when the model's performance on the validation set starts to degrade, preventing overfitting.\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 8)\n"
     ]
    }
   ],
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.\n",
    "\n",
    "Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function to discourage large model coefficients. Common regularization techniques include:\n",
    "\n",
    "L1 (Lasso) regularization: adds a term to the loss function that is proportional to the absolute value of the model coefficients, encouraging sparse models.\n",
    "L2 (Ridge) regularization: adds a term to the loss function that is proportional to the square of the model coefficients, encouraging small model coefficients.\n",
    "Dropout regularization: randomly drops out neurons during training, preventing the model from relying too heavily on any single neuron.\n",
    "Early stopping: stops training when the model's performance on the validation set starts to degrade, preventing overfitting.\n",
    "Here is some Python code to demonstrate regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "789159ed-5928-400c-be89-6c1f408557bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3528860099.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f5523a-befe-4fd8-86df-0cacba26f517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7754fe6c-797f-41a2-bda3-18fe2db273f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (3.4.1)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras) (0.12.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from keras) (22.0)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: ml-dtypes in /opt/conda/lib/python3.10/site-packages (from keras) (0.4.0)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras) (3.7.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from optree->keras) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras) (2.13.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e8968bc-98f1-4fd2-875c-a73dfcc2e46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (0.12.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from optree) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "944fafc8-cb88-4069-bfca-f6623eb751ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Lasso, Ridge\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_boston\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the Boston housing dataset\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/datasets/__init__.py:156\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_boston\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    106\u001b[0m     msg \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m        `load_boston` has been removed from scikit-learn since version 1.2.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     )\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[name]\n",
      "\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Boston housing dataset\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# L1 Regularization (Lasso)\n",
    "lasso_model = Lasso(alpha=0.1)  # alpha is the regularization strength\n",
    "lasso_model.fit(X_train, y_train)\n",
    "print(\"Lasso Coefficients:\", lasso_model.coef_)\n",
    "\n",
    "# L2 Regularization (Ridge)\n",
    "ridge_model = Ridge(alpha=1.0)  # alpha is the regularization strength\n",
    "ridge_model.fit(X_train, y_train)\n",
    "print(\"Ridge Coefficients:\", ridge_model.coef_)\n",
    "\n",
    "# Compare the performance of the models\n",
    "print(\"Lasso Score:\", lasso_model.score(X_test, y_test))\n",
    "print(\"Ridge Score:\", ridge_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6c1f5a-f6ce-42df-88a3-446c4f416511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
